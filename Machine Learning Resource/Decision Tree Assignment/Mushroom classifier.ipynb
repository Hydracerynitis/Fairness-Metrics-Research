{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a decision tree classifier to predicate whether a mushroom is poisonous based on its feature, the data used for training is from \"agaricus-lepiota\" database.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:19:55.315563700Z",
     "start_time": "2023-09-25T04:19:54.887078300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of the dataset is edibility ['e', 'p']\n",
      "The feature of the dataset are: \n",
      "cap-shape ['x', 'b', 's', 'f', 'k', 'c']\n",
      "cap-surface ['s', 'y', 'f', 'g']\n",
      "cap-color ['n', 'y', 'w', 'g', 'e', 'p', 'b', 'c']\n",
      "bruises ['t', 'f']\n",
      "odor ['p', 'a', 'l', 'n', 'f', 'c', 'm']\n",
      "gill-attachment ['f', 'a']\n",
      "gill-spacing ['c', 'w']\n",
      "gill-size ['n', 'b']\n",
      "gill-color ['k', 'n', 'g', 'p', 'w', 'h', 'u', 'r', 'y']\n",
      "stalk-shape ['e', 't']\n",
      "stalk-root ['e', 'c', 'b', 'r']\n",
      "stalk-surface-above-ring ['s', 'f', 'k', 'y']\n",
      "stalk-surface-below-ring ['s', 'f', 'y', 'k']\n",
      "stalk-color-above-ring ['w', 'g', 'p', 'n', 'b', 'c', 'y']\n",
      "stalk-color-below-ring ['w', 'p', 'g', 'b', 'n', 'c', 'y']\n",
      "veil-type ['p']\n",
      "veil-color ['w', 'y']\n",
      "ring-number ['o', 't', 'n']\n",
      "ring-type ['p', 'e', 'l', 'n']\n",
      "spore-print-color ['k', 'n', 'u', 'h', 'r', 'w']\n",
      "population ['s', 'n', 'a', 'v', 'y', 'c']\n",
      "habitat ['u', 'g', 'm', 'd', 'p', 'l']\n",
      "-------\n",
      "There is 3985 examples int the trainning set.\n",
      "There is 1659 examples int the testing set.\n"
     ]
    }
   ],
   "source": [
    "from random import *\n",
    "feature_list=[\"cap-shape\",\"cap-surface\",\"cap-color\",\"bruises\",\"odor\",\"gill-attachment\",\"gill-spacing\",\"gill-size\",\"gill-color\",\"stalk-shape\",\"stalk-root\",\n",
    "              \"stalk-surface-above-ring\",\"stalk-surface-below-ring\",\"stalk-color-above-ring\",\"stalk-color-below-ring\",\"veil-type\",\"veil-color\",\"ring-number\",\n",
    "              \"ring-type\",\"spore-print-color\",\"population\",\"habitat\"]\n",
    "#Set up the list of the names of feature \n",
    "feature_dict=dict()\n",
    "for feature in feature_list:\n",
    "    feature_dict[feature]=[]\n",
    "#Establish the dictionary to store all possible values for each feature.\n",
    "file=open(\"agaricus-lepiota.data\",\"r\")\n",
    "dataset=[]\n",
    "for line in file.readlines():\n",
    "    line=line.removesuffix(\"\\n\")\n",
    "    example=line.split(\",\")\n",
    "    label=example[0]\n",
    "    example=example[1:]\n",
    "    if \"?\" in example:\n",
    "        continue\n",
    "    for i in range(len(feature_list)):\n",
    "        feature=feature_list[i]\n",
    "        if example[i] not in feature_dict[feature]:\n",
    "            feature_dict[feature].append(example[i])\n",
    "    dataset.append((example,label))\n",
    "#Read through the dataset, store any seen feature value into the dictionary. If the data entry has missing value ignore it.\n",
    "print(\"The label of the dataset is edibility [\\'e\\', \\'p\\']\")\n",
    "#label with e value is edible, while label with p value is poisonous.\n",
    "print(\"The feature of the dataset are: \")\n",
    "for i in feature_dict:\n",
    "    print(i, feature_dict[i])\n",
    "seed(100)\n",
    "testset=[]\n",
    "trainset=[]\n",
    "for i in dataset:\n",
    "    if random()<=0.3:\n",
    "        testset.append(i)\n",
    "    else:\n",
    "        trainset.append(i)\n",
    "#Data for training and Data for testing need to be completely exclusive to each other. In this model, the training set and testing set are split randomly at 70:30 ratio.\n",
    "print(\"-------\")\n",
    "print(\"There is {0} examples int the trainning set.\".format(len(trainset)))\n",
    "print(\"There is {0} examples int the testing set.\".format(len(testset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:19:55.315563700Z",
     "start_time": "2023-09-25T04:19:55.042471600Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feature(featture_index):\n",
    "    return feature_list[featture_index]\n",
    "#This is the method to get the name of the feature with its index\n",
    "def get_feature_type(feature_index):\n",
    "    return feature_dict[get_feature(feature_index)]\n",
    "#This is the method to get all possible value of the feature with its index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code block processes the agaricus-lepiota.data file and seperate the examples in the dataset into the testing set that reserved to test the model and the training set reserved to train the model.\n",
    "\n",
    "It also extract all features of the data set in `feature_list`. For each feature, it records all posible types of the feature into `feature_dict`. The second code block provide utility method to transfer feature's index in `feature_list` to the name of respective feature and to all its possible type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:19:55.315563700Z",
     "start_time": "2023-09-25T04:19:55.060897200Z"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "#Import abstract class and Enumeration for my decision tree model implementation\n",
    "class PredictType(Enum):\n",
    "    Result = 0\n",
    "    Condition = 1\n",
    "\n",
    "class PredictModel(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def predict(self,features):\n",
    "        raise NotImplementedError\n",
    "#The supperclass of all decision tree nodes\n",
    "    \n",
    "class ResultPredict(PredictModel):\n",
    "    def __init__(self,result) -> None:\n",
    "        super().__init__()\n",
    "        self.__result=result\n",
    "    def predict(self, features):\n",
    "        return PredictType.Result,self.__result\n",
    "    def __str__(self) -> str:\n",
    "        return \"end(\"+self.__result+\")\"\n",
    "#The leaf nodes of the decision tree, it return its predicted label value\n",
    "\n",
    "class ConditionPredict(PredictModel):\n",
    "    def __init__(self,feature_index) -> None:\n",
    "        super().__init__()\n",
    "        self.__feature_index=feature_index\n",
    "    \n",
    "    def predict(self, features):\n",
    "        return PredictType.Condition,features[self.__feature_index]\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return get_feature(self.__feature_index)\n",
    "#The internal nodes of the decision tree, it return the index of feature that it want to split entry into.\n",
    "        \n",
    "class DecisionTreeNode:\n",
    "    def __init__(self,name,data=None) -> None:\n",
    "        self.__name=name\n",
    "        self.__childs=[]\n",
    "        self.__parent=None\n",
    "        self.__predictModel: PredictModel = None\n",
    "        self.__data=data\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.__data\n",
    "    @property\n",
    "    def child(self):\n",
    "        return self.__childs\n",
    "    def addChild(self,node):\n",
    "        self.__childs.append(node)\n",
    "        node.__parent=self\n",
    "    \n",
    "    def setPredictModel(self,model):\n",
    "        if isinstance(model,PredictModel):\n",
    "            self.__predictModel=model\n",
    "        return self\n",
    "\n",
    "    def predict(self,features):\n",
    "        if self.__predictModel==None:\n",
    "            raise ValueError\n",
    "        predict_type,predict_result=self.__predictModel.predict(features)\n",
    "        if predict_type==PredictType.Result:\n",
    "            return predict_result\n",
    "        else:\n",
    "            for child_node in self.__childs:\n",
    "                if child_node.name==predict_result:\n",
    "                    return child_node.predict(features)\n",
    "            return \"p\"\n",
    "                \n",
    "    def print_tree(self):\n",
    "        print(self)\n",
    "        for i in self.__childs:\n",
    "            i.print_tree()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.__parent == None:\n",
    "            return self.__name+ (\"\" if self.__predictModel==None else \"-\"+str(self.__predictModel))\n",
    "        else:\n",
    "            return str(self.__parent)+\"-\"+self.__name+(\"\" if self.__predictModel==None else \"-\"+str(self.__predictModel))\n",
    "#This is the decision tree model, which I use node list as its data structure. Its prediction model is the node of decision tree that it represent. It would also contain the dataset it has to process to make training process much easier to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block provides clases that support a decision tree prediction model. I choose to use linked tree as its data structure because it's easy to expand its range and propogate a change across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:19:55.315563700Z",
     "start_time": "2023-09-25T04:19:55.091142300Z"
    }
   },
   "outputs": [],
   "source": [
    "def countLabel(data):\n",
    "    count_e=0\n",
    "    count_p=0\n",
    "    for i in data:\n",
    "        if i[1]==\"e\":\n",
    "            count_e+=1\n",
    "        elif i[1]==\"p\":\n",
    "            count_p+=1\n",
    "    return count_e,count_p\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    count_e,count_p=countLabel(data)\n",
    "    if count_e==0 or count_p==0:\n",
    "        return 0\n",
    "    else:\n",
    "        probability_e=count_e/len(data)\n",
    "        probability_p=count_p/len(data)\n",
    "        return -probability_e*math.log2(probability_e)-probability_p*math.log2(probability_p)\n",
    "\n",
    "#calculate the entropy of a set of data, the less entropy it's much easier to predict. So we want to choose the feature which split dataset according to value of it will generate several dataset that has a sum of lower entropy than before.\n",
    "\n",
    "def getMostAccurate(data):\n",
    "    count_e,count_p=countLabel(data)\n",
    "    return \"e\" if count_e>count_p else \"p\"\n",
    "\n",
    "#Get the most accurate prediction for certian data set.\n",
    "\n",
    "def test_split_feature(data,feature_index):\n",
    "    total_len=len(data)\n",
    "    data=list(data)\n",
    "    feature_type=get_feature_type(feature_index)\n",
    "    split_set=[]\n",
    "    for i in range(len(feature_type)):\n",
    "        split=[]\n",
    "        individual_class=feature_type[i]\n",
    "        for sample in data:\n",
    "            if sample[0][feature_index]==individual_class:\n",
    "                split.append(sample)\n",
    "        for example in split:\n",
    "            data.remove(example)\n",
    "        if len(split)!=0:\n",
    "            split_set.append((split,individual_class))\n",
    "    entropy=0\n",
    "    for i in split_set:\n",
    "        entropy+=len(i)/total_len*calculate_entropy(i[0])\n",
    "    return entropy,split_set\n",
    "#Calculate the change in entropy after splitting dataset according to a feature.\n",
    "\n",
    "def EntropySort(elm:DecisionTreeNode):\n",
    "    return calculate_entropy(elm.data)\n",
    "\n",
    "#Sort function based the entropy of data in the DecisionTreeNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:19:55.315563700Z",
     "start_time": "2023-09-25T04:19:55.121506200Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "remaining_feature=[]\n",
    "notProcessedNode=[]\n",
    "#remaining_feature store all feature that not yet to be split on. while notProcessingNode stored decision tree nodes which data set has not yet be processed and decided a model\n",
    "\n",
    "def train_model(data):\n",
    "    remaining_feature.clear()\n",
    "    notProcessedNode.clear()\n",
    "    for i in range(22):\n",
    "        remaining_feature.append(i)\n",
    "    head=DecisionTreeNode(\"Head\",data)\n",
    "    notProcessedNode.append(head)\n",
    "    train_find_predict_model()\n",
    "    return head\n",
    "def train_find_predict_model():\n",
    "    decision_tree_node=notProcessedNode[0]\n",
    "    data=decision_tree_node.data\n",
    "    if len(data)<=3:\n",
    "        decision_tree_node.setPredictModel(ResultPredict(getMostAccurate(data)))\n",
    "    else:\n",
    "        base_entropy=calculate_entropy(data)\n",
    "        infogain=0\n",
    "        best_feature=-1\n",
    "        best_split_data=[]\n",
    "        for i in remaining_feature:\n",
    "            entropy,split_data=test_split_feature(data,i)\n",
    "            if infogain < base_entropy-entropy:\n",
    "                infogain=base_entropy-entropy\n",
    "                best_feature=i\n",
    "                best_split_data=split_data\n",
    "        if best_feature==-1:\n",
    "            decision_tree_node.setPredictModel(ResultPredict(getMostAccurate(data)))\n",
    "        else:\n",
    "            decision_tree_node.setPredictModel(ConditionPredict(best_feature))\n",
    "            remaining_feature.remove(best_feature)\n",
    "            for i in best_split_data:\n",
    "                new_node=DecisionTreeNode(i[1],i[0])\n",
    "                decision_tree_node.addChild(new_node)\n",
    "                notProcessedNode.append(new_node)\n",
    "    notProcessedNode.remove(decision_tree_node)\n",
    "    if len(notProcessedNode)<=0:\n",
    "        return\n",
    "    else:\n",
    "        if len(remaining_feature)<=1:\n",
    "            for node in notProcessedNode:\n",
    "                node.setPredictModel(ResultPredict(getMostAccurate(node.data)))\n",
    "            return\n",
    "        else:\n",
    "            notProcessedNode.sort(key=EntropySort,reverse=True)\n",
    "            train_find_predict_model()\n",
    "#The training algorithm is to find the best feature to split data stored inside a node. If no feature can reduce the entropy, then the node will become a leaf that return the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block is the algorithm to train a decision tree model given a data set. For each split, the algorithm will look for the tree node that has the most entropy subsets. It would then use information gain criteria to choose the best feature to split its subsets. The algorithm will not choose the same feature twice to split, and will not further split a node if it sentropy can not be improved or its subset is too small.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:19:56.722989200Z",
     "start_time": "2023-09-25T04:19:55.153890700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained decision model tree is:\n",
      "Head-odor\n",
      "Head-odor-p-end(p)\n",
      "Head-odor-a-end(e)\n",
      "Head-odor-l-end(e)\n",
      "Head-odor-n-spore-print-color\n",
      "Head-odor-n-spore-print-color-k-end(e)\n",
      "Head-odor-n-spore-print-color-n-end(e)\n",
      "Head-odor-n-spore-print-color-r-end(p)\n",
      "Head-odor-n-spore-print-color-w-end(e)\n",
      "Head-odor-f-end(p)\n",
      "Head-odor-c-end(p)\n",
      "Head-odor-m-end(p)\n",
      "-------------\n",
      "A random sample from the test set has feature of ['f', 's', 'g', 't', 'f', 'f', 'c', 'b', 'p', 't', 'b', 's', 's', 'w', 'w', 'p', 'w', 'o', 'p', 'h', 's', 'g']\n",
      "Its label is p\n",
      "Based on the feature, the model will predict the label is p\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "i=0\n",
    "seed()\n",
    "while i<2000:\n",
    "    train_data.append(trainset[randint(0,len(trainset)-1)])\n",
    "    i+=1\n",
    "#Getting training data by bootstrapping to reduce overfitting\n",
    "\n",
    "model=train_model(train_data)\n",
    "print(\"The trained decision model tree is:\")\n",
    "model.print_tree()\n",
    "print(\"-------------\")\n",
    "random_test=testset[randint(0,len(testset)-1)]\n",
    "print(\"A random sample from the test set has feature of {0}\".format(random_test[0]))\n",
    "print(\"Its label is {0}\".format(random_test[1]))\n",
    "print(\"Based on the feature, the model will predict the label is {0}\".format(model.predict(random_test[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clock block prepare training samples and execute the `train_model` function. It uses a randomized seed to bootstrap training smaples to explore different possible models. The sample size is set to 2000\n",
    "\n",
    "There is one thing particular across all decision tree models that their first split will always be \"odor\" and none odor (n) will be the only subset that algorithm think required further spliting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the decision tree with limited depth specified as a parameter of the training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:19:56.746420200Z",
     "start_time": "2023-09-25T04:19:56.738621700Z"
    }
   },
   "outputs": [],
   "source": [
    "depth=0\n",
    "def train_model_depth_control(data,stopped_depth):\n",
    "    global depth\n",
    "    remaining_feature.clear()\n",
    "    notProcessedNode.clear()\n",
    "    for i in range(22):\n",
    "        remaining_feature.append(i)\n",
    "    depth=stopped_depth\n",
    "    head=DecisionTreeNode(\"Head\",data)\n",
    "    notProcessedNode.append(head)\n",
    "    train_find_predict_model_depth_control()\n",
    "    return head\n",
    "def train_find_predict_model_depth_control():\n",
    "    global depth\n",
    "    decision_tree_node=notProcessedNode[0]\n",
    "    data=decision_tree_node.data\n",
    "    if len(data)<=3:\n",
    "        decision_tree_node.setPredictModel(ResultPredict(getMostAccurate(data)))\n",
    "    else:\n",
    "        base_entropy=calculate_entropy(data)\n",
    "        infogain=0\n",
    "        best_feature=-1\n",
    "        best_split_data=[]\n",
    "        for i in remaining_feature:\n",
    "            entropy,split_data=test_split_feature(data,i)\n",
    "            if infogain < base_entropy-entropy:\n",
    "                infogain=base_entropy-entropy\n",
    "                best_feature=i\n",
    "                best_split_data=split_data\n",
    "        if best_feature==-1:\n",
    "            decision_tree_node.setPredictModel(ResultPredict(getMostAccurate(data)))\n",
    "        else:\n",
    "            decision_tree_node.setPredictModel(ConditionPredict(best_feature))\n",
    "            remaining_feature.remove(best_feature)\n",
    "            depth-=1\n",
    "            for i in best_split_data:\n",
    "                new_node=DecisionTreeNode(i[1],i[0])\n",
    "                decision_tree_node.addChild(new_node)\n",
    "                notProcessedNode.append(new_node)\n",
    "    notProcessedNode.remove(decision_tree_node)\n",
    "    if len(notProcessedNode)<=0:\n",
    "        return\n",
    "    else:\n",
    "        if len(remaining_feature)<=1 or depth<=0:\n",
    "            for node in notProcessedNode:\n",
    "                node.setPredictModel(ResultPredict(getMostAccurate(node.data)))\n",
    "            return\n",
    "        else:\n",
    "            notProcessedNode.sort(key=EntropySort,reverse=True)\n",
    "            train_find_predict_model_depth_control()\n",
    "#added global variable depth to limit the exploration of training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:19:58.639340600Z",
     "start_time": "2023-09-25T04:19:56.746420200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision model tree with depth 4 is:\n",
      "Head-odor\n",
      "Head-odor-p-end(p)\n",
      "Head-odor-a-end(e)\n",
      "Head-odor-l-end(e)\n",
      "Head-odor-n-veil-color\n",
      "Head-odor-n-veil-color-w-gill-attachment\n",
      "Head-odor-n-veil-color-w-gill-attachment-f-veil-type\n",
      "Head-odor-n-veil-color-w-gill-attachment-f-veil-type-p-end(e)\n",
      "Head-odor-n-veil-color-y-end(p)\n",
      "Head-odor-f-end(p)\n",
      "Head-odor-c-end(p)\n",
      "Head-odor-m-end(p)\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "i=0\n",
    "seed()\n",
    "while i<2000:\n",
    "    train_data.append(trainset[randint(0,len(trainset)-1)])\n",
    "    i+=1\n",
    "\n",
    "model=train_model_depth_control(train_data,4)\n",
    "print(\"The decision model tree with depth 4 is:\")\n",
    "model.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:20:00.216368900Z",
     "start_time": "2023-09-25T04:19:58.646822700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision model tree with depth 3 is:\n",
      "Head-odor\n",
      "Head-odor-p-end(p)\n",
      "Head-odor-a-end(e)\n",
      "Head-odor-l-end(e)\n",
      "Head-odor-n-veil-color\n",
      "Head-odor-n-veil-color-w-gill-attachment\n",
      "Head-odor-n-veil-color-w-gill-attachment-f-end(e)\n",
      "Head-odor-n-veil-color-y-end(p)\n",
      "Head-odor-f-end(p)\n",
      "Head-odor-c-end(p)\n",
      "Head-odor-m-end(p)\n"
     ]
    }
   ],
   "source": [
    "model=train_model_depth_control(train_data,3)\n",
    "print(\"The decision model tree with depth 3 is:\")\n",
    "model.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:20:01.591991300Z",
     "start_time": "2023-09-25T04:20:00.216368900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision model tree with depth 2 is:\n",
      "Head-odor\n",
      "Head-odor-p-end(p)\n",
      "Head-odor-a-end(e)\n",
      "Head-odor-l-end(e)\n",
      "Head-odor-n-veil-color\n",
      "Head-odor-n-veil-color-w-end(e)\n",
      "Head-odor-n-veil-color-y-end(p)\n",
      "Head-odor-f-end(p)\n",
      "Head-odor-c-end(p)\n",
      "Head-odor-m-end(p)\n"
     ]
    }
   ],
   "source": [
    "model=train_model_depth_control(train_data,2)\n",
    "print(\"The decision model tree with depth 2 is:\")\n",
    "model.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These code blocks provdie a training algoeithm that have depth control implemented. `stopped_depth` argument will further limit the split that the algorithm will make.\n",
    "\n",
    "The decision tree with value of `stopped_death` being 4,3,2 have their representation printed out repectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the accuracy of the decision tree model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:20:01.618193800Z",
     "start_time": "2023-09-25T04:20:01.591991300Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model:DecisionTreeNode, data):\n",
    "    true_p=0\n",
    "    true_n=0\n",
    "    false_p=0\n",
    "    false_n=0\n",
    "    for i in data:\n",
    "        predict=model.predict(i[0])\n",
    "        if predict==\"e\":\n",
    "            if predict==i[1]:\n",
    "                true_p+=1\n",
    "            else:\n",
    "                false_p+=1\n",
    "        elif predict==\"p\":\n",
    "            if predict==i[1]:\n",
    "                true_n+=1\n",
    "            else:\n",
    "                false_n+=1\n",
    "        else:\n",
    "            false_n+=1\n",
    "    accuracy=(true_p+true_n)/(true_n+true_p+false_n+false_p)\n",
    "    precision=true_p/(true_p+false_p)\n",
    "    recall=true_p/(true_p+false_n)\n",
    "    F1_score=2*(precision*recall)/(precision+recall)\n",
    "    return true_p,true_n,false_p,false_n,accuracy,precision,recall,F1_score\n",
    "#Model Testing functions,producing accuracy (ratio of the prediction is right), precision (ratio of the prediction is right when the prediction is positive), recall (ratio of the prediction is right when the label is positive) and F1-socre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block provide the test procedure that take decision tree model and testing set, compare the predicted label from the model and the real label of data, and output numbers of true positive cases, true negative cases, false positive cases, false negative case, calculated model's accuracy, precision, recall and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:20:03.904302700Z",
     "start_time": "2023-09-25T04:20:01.605745700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When testing 1000 cases, the model made 630 true positive, 369 true negative, 0 false positive and 1 false negative\n",
      "In this test, the model as 0.999 accuracy, 1.0 precision and 0.9984152139461173 recall. The F1 score evaluation of the model is 0.9992069785884219\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "i=0\n",
    "seed()\n",
    "while i<2000:\n",
    "    train_data.append(trainset[randint(0,len(trainset)-1)])\n",
    "    i+=1\n",
    "\n",
    "model=train_model(train_data)\n",
    "j=0\n",
    "test_data=[]\n",
    "while j<1000:\n",
    "    test_data.append(testset[randint(0,len(testset)-1)])\n",
    "    j+=1\n",
    "#Testing set is also obtained through bootstrap\n",
    "true_p,true_n,false_p,false_n,accuracy,precision,recall,F1_score=test_model(model,test_data)\n",
    "print(\"When testing 1000 cases, the model made {0} true positive, {1} true negative, {2} false positive and {3} false negative\".format(true_p,true_n,false_p,false_n))\n",
    "print(\"In this test, the model as {0} accuracy, {1} precision and {2} recall. The F1 score evaluation of the model is {3}\".format(accuracy,precision,recall,F1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block I use the non-depth-control algorithm training model to get rough estimation of the model's performance. I use randomized seed to booststrap 2000 examples of training set and 1000 examples of testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate the perfromance of decision tree with Five-fold-cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:20:03.934879Z",
     "start_time": "2023-09-25T04:20:03.904302700Z"
    }
   },
   "outputs": [],
   "source": [
    "def five_fold_cross_validation(data):\n",
    "    folds=[[],[],[],[],[]]\n",
    "    data=list(data)\n",
    "    while len(data)>0:\n",
    "        fold_slot=list(range(5))\n",
    "        while len(fold_slot)>0:\n",
    "            seed()\n",
    "            example=data.pop(randint(0,len(data)-1))\n",
    "            slot=fold_slot.pop(randint(0,len(fold_slot)-1))\n",
    "            folds[slot].append(example)\n",
    "    accuracy_list=[]\n",
    "    for i in range(5):\n",
    "        valid_fold=folds[i]\n",
    "        train_fold=[]\n",
    "        for f in folds:\n",
    "            if f!=valid_fold:\n",
    "                train_fold+=f\n",
    "        model=train_model_depth_control(train_fold,1)\n",
    "        evaluation=test_model(model,valid_fold)\n",
    "        accuracy_list.append(evaluation[4])\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would use five fold cross validation as the evaluation for my decision tree algorithm\n",
    "\n",
    "This code block provide an implementation of five fold cross validation on decision tree algorithm with `stopped_depth` being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T04:20:08.799999700Z",
     "start_time": "2023-09-25T04:20:03.919226600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation results from the five fold cross validation is : [0.98, 0.9875, 0.98, 0.985, 0.9925]\n",
      "The average validation accuracy for the model is 0.985\n",
      "The test accuracy for the model is 0.994, its F1-score is 0.995334370139969\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "i=0\n",
    "seed()\n",
    "while i<2000:\n",
    "    train_data.append(trainset[randint(0,len(trainset)-1)])\n",
    "    i+=1\n",
    "fold_result=five_fold_cross_validation(train_data)\n",
    "print(\"The validation results from the five fold cross validation is : {0}\".format(fold_result))\n",
    "print(\"The average validation accuracy for the model is {0}\".format(sum(fold_result)/len(fold_result)))\n",
    "j=0\n",
    "test_data=[]\n",
    "while j<1000:\n",
    "    test_data.append(testset[randint(0,len(testset)-1)])\n",
    "    j+=1\n",
    "model=train_model_depth_control(train_data,1)\n",
    "evaluation=test_model(model,test_data)\n",
    "print(\"The test accuracy for the model is {0}, its F1-score is {1}\".format(evaluation[4],evaluation[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block run a five-fold cross validation on the algorithm to get the average validation accuracy, and then compare it to the test accuracy and F1-score.\n",
    "\n",
    "The result show that they are very similar with each other, meaning the dection tree algorithm can indeed secure a 99% accuracy on the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
